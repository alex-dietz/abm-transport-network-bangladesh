{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libaries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "#opress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from shapely.geometry import LineString, Point\n",
    "import math\n",
    "import geopandas as gpd\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load cleaned road data\n",
    "roads = pd.read_csv('../data/raw/_roads3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coordinate(lrp, road_name,search_lon):\n",
    "    \"\"\"\n",
    "    This function looks up the coordinates of a given lrp and road name.\n",
    "    \"\"\"\n",
    "    mask = (roads['lrp'] == lrp) & (roads['road'] == road_name)\n",
    "    matching_rows = roads[mask]\n",
    "    \n",
    "    if not matching_rows.empty:\n",
    "        if search_lon:\n",
    "            return matching_rows.iloc[0]['lon']\n",
    "        else:\n",
    "            return matching_rows.iloc[0]['lat']\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_linestring(row):\n",
    "    \"\"\"\n",
    "    Creates a linestring from the start and end point of a row\n",
    "    \"\"\"\n",
    "    start_point = (row['lon_start'], row['lat_start'])\n",
    "    end_point = (row['lon_end'], row['lat_end'])\n",
    "    return LineString([start_point, end_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_road_data(path):\n",
    "    \"\"\"\n",
    "    This function cleans the road data and returns a dataframe with the cleaned data.\n",
    "    \"\"\"\n",
    "    \n",
    "    #open file\n",
    "    file = open(path, 'r')\n",
    "    soup = BeautifulSoup(file)\n",
    "    #Search Data Table\n",
    "    table = soup.find('table', attrs={'class':'clsTbl'})\n",
    "    table_body = table.find('tbody')\n",
    "    rows = table_body.find_all('tr')\n",
    "    data = []\n",
    "    #loop through rows\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [ele.text.strip() for ele in cols]\n",
    "        data.append([ele for ele in cols if ele]) \n",
    "    df_clean = pd.DataFrame(data)\n",
    "    #drop first 4 row without data\n",
    "    df = df_clean.drop(df_clean.index[0:4])\n",
    "\n",
    "    #drop last 2 row without data\n",
    "    df = df.drop(df.index[-2:])\n",
    "    #make first row to header\n",
    "    new_header = df.iloc[0] \n",
    "    #take the data less the header row\n",
    "    df = df[1:]\n",
    "    df.columns = new_header\n",
    "    #store headers in list\n",
    "    headers = df.columns.values.tolist()\n",
    "    #drop last 2 elements of headers list\n",
    "    headers = headers[:-2]\n",
    "    #add 'link_no', 'name' to headers\n",
    "    headers.insert(0, 'link_no')\n",
    "    headers.insert(1, 'name')\n",
    "    #rename headers to headers\n",
    "    df.columns = headers\n",
    "    #remove columns with None header    \n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    #rename headers\n",
    "    headers = df.columns.values.tolist()\n",
    "    headers[2] = 'LRP_Start'\n",
    "    headers[3] = 'Offset_Start'\n",
    "    headers[4] = 'Chainage_Start'\n",
    "    headers[5] = 'LRP_End'\n",
    "    headers[6] = 'Offset_End'\n",
    "    headers[7] = 'Chainage_End'\n",
    "    \n",
    "    df_final = df\n",
    "    #set final headers\n",
    "    df_final.columns = headers\n",
    "    #remove rows with 'NS' in 'Heavy Truck' column\n",
    "    df_final = df_final[df_final['Heavy Truck'] != 'NS']\n",
    "    \n",
    "   \n",
    "    return df_final\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing: N501\n",
      "Parsing: N604\n",
      "Parsing: N101\n",
      "Parsing: N204\n",
      "Parsing: N508\n",
      "Parsing: N211\n",
      "Parsing: N303\n",
      "Parsing: N703\n",
      "Parsing: N108\n",
      "Parsing: N2\n",
      "Parsing: N406\n",
      "Parsing: N514\n",
      "Parsing: N126\n",
      "Parsing: N111\n",
      "Parsing: N123\n",
      "Parsing: N208\n",
      "Parsing: N511\n",
      "Parsing: N403\n",
      "Parsing: N7\n",
      "Parsing: N706\n",
      "Parsing: N713\n",
      "Parsing: N504\n",
      "Parsing: N518\n",
      "Parsing: N802\n",
      "Parsing: N104\n",
      "Parsing: N801\n",
      "Parsing: N107\n",
      "Parsing: N129\n",
      "Parsing: N507\n",
      "Parsing: N602\n",
      "Parsing: N710\n",
      "Parsing: N808\n",
      "Parsing: N705\n",
      "Parsing: N120\n",
      "Parsing: N4\n",
      "Parsing: N520\n",
      "Parsing: N112\n",
      "Parsing: N405\n",
      "Parsing: N1\n",
      "Parsing: N125\n",
      "Parsing: N517\n",
      "Parsing: N212\n",
      "Parsing: N207\n",
      "Parsing: N8\n",
      "Parsing: N102\n",
      "Parsing: N804\n",
      "Parsing: N540\n",
      "Parsing: N709\n",
      "Parsing: N715\n",
      "Parsing: N309\n",
      "Parsing: N502\n",
      "Parsing: N519\n",
      "Parsing: N105\n",
      "Parsing: N803\n",
      "Parsing: N712\n",
      "Parsing: N119\n",
      "Parsing: N505\n",
      "Parsing: N6\n",
      "Parsing: N402\n",
      "Parsing: N510\n",
      "Parsing: N209\n",
      "Parsing: N707\n",
      "Parsing: N110\n",
      "Parsing: N109\n",
      "Parsing: N702\n",
      "Parsing: N127\n",
      "Parsing: N515\n",
      "Parsing: N407\n",
      "Parsing: N3\n",
      "Parsing: N302\n",
      "Parsing: N210\n",
      "Parsing: N509\n",
      "Parsing: N806\n",
      "Parsing: N205\n",
      "Parsing: N605\n",
      "Parsing: N503\n",
      "Parsing: N206\n",
      "Parsing: N708\n",
      "Parsing: N805\n",
      "Parsing: N103\n",
      "Parsing: N213\n",
      "Parsing: N516\n",
      "Parsing: N124\n",
      "Parsing: N404\n",
      "Parsing: N701\n",
      "Parsing: N704\n",
      "Parsing: N809\n",
      "Parsing: N401\n",
      "Parsing: N5\n",
      "Parsing: N513\n",
      "Parsing: N506\n",
      "Parsing: N711\n",
      "Parsing: N603\n",
      "Parsing: N106\n",
      "Parsing: N203\n",
      "Parsing: N128\n",
      "Parsing: N408\n"
     ]
    }
   ],
   "source": [
    "#remove road_data.csv if exists\n",
    "if os.path.isfile('../data/processed/road_data.csv'):\n",
    "    os.remove('../data/processed/road_data.csv')\n",
    "#loop through all files in data folder that start with N\n",
    "for file in os.listdir('../data/raw/road_data'):\n",
    "    if file.startswith('N'):\n",
    "        #define road_name\n",
    "        road_name = file.split('.')[0]        \n",
    "        print(\"Parsing: \"+road_name)\n",
    "        #skip N203 because no data\n",
    "        if road_name == 'N203':\n",
    "            continue\n",
    "        #read data            \n",
    "        df = clean_road_data('../data/raw/road_data/'+file)\n",
    "        #if dataframe is empty or only one entry, skip\n",
    "        \n",
    "        #keep only first occurence if link_no is duplicated\n",
    "        df = df.drop_duplicates(subset='link_no', keep='first')\n",
    "        if df.empty or len(df) == 1:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        \n",
    "        #add road name column\n",
    "        df['road_name'] = road_name\n",
    "        #add coordinates\n",
    "        df['lon_start'] = df.apply(lambda row: lookup_coordinate(row['LRP_Start'], row['road_name'],True), axis=1)\n",
    "        df['lat_start'] = df.apply(lambda row: lookup_coordinate(row['LRP_Start'], row['road_name'],False), axis=1)\n",
    "        df['lon_end'] = df.apply(lambda row: lookup_coordinate(row['LRP_End'], row['road_name'],True), axis=1)\n",
    "        df['lat_end'] = df.apply(lambda row: lookup_coordinate(row['LRP_End'], row['road_name'],False), axis=1)\n",
    "        #skip if coordinates are None\n",
    "        if df['lon_start'].isnull().values.any() or df['lat_start'].isnull().values.any() or df['lon_end'].isnull().values.any() or df['lat_end'].isnull().values.any():\n",
    "            continue\n",
    "        #find mid point for water stations\n",
    "        df['lon_mid'] = (df['lon_start'] + df['lon_end'])/2\n",
    "        df['lat_mid'] = (df['lat_start'] + df['lat_end'])/2\n",
    "        df['mid_geometry'] = df.apply(lambda row: Point(row['lon_mid'], row['lat_mid']), axis=1)\n",
    "        \n",
    "        # Create a new 'geometry' column containing LINESTRING objects\n",
    "        df['geometry'] = df.apply(create_linestring, axis=1)\n",
    "        \n",
    "        #write to global csv if exists\n",
    "        #catch error if cannot write to csv\n",
    "        try:\n",
    "            if os.path.isfile('../data/processed/road_data.csv'):\n",
    "                df.to_csv('../data/processed/road_data.csv', mode='a', header=False,index=False)\n",
    "            #else create new file\n",
    "            else:\n",
    "                df.to_csv('../data/processed/road_data.csv', mode='a', header=True,index=False)\n",
    "        except:\n",
    "            print(\"Could not write to csv\")\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Vulnerablity </h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Water Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_segments = pd.read_csv('../data/processed/road_data.csv')\n",
    "LRP_data = pd.read_csv('../data/raw/_roads3.csv')\n",
    "water_data = pd.read_excel('../data/raw/flood_data/geolocations_stations.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excess_water(segment):\n",
    "    if math.isnan(segment['Highest Water Level']):\n",
    "        return segment['Water Level ']-segment['Danger Level ']\n",
    "    else:\n",
    "        return segment['Highest Water Level']-segment['Danger Level ']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create Excess Water Level\n",
    "water_data['Excess Water Level'] = water_data.apply(excess_water,axis=1)\n",
    "water_data['geometry'] = water_data.apply(lambda row: Point(row['Longitude '], row['Latitude ']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#road_segments to gdf\n",
    "road_segments['mid_geometry'] = road_segments['mid_geometry'].astype(str).apply(wkt.loads)\n",
    "\n",
    "road_segments = gpd.GeoDataFrame(road_segments, geometry='mid_geometry')\n",
    "#water_data to gdf\n",
    "\n",
    "water_data = gpd.GeoDataFrame(water_data, geometry='geometry')\n",
    "# conduct Spatial Join using Nearest Neighbour\n",
    "joined = gpd.sjoin_nearest(road_segments, water_data, how=\"inner\", lsuffix=\"\")\n",
    "\n",
    "road_segments_water = joined.drop(['index_right', 'Station Name ', 'River Name ', 'Division ',\n",
    "       'District ', 'Upazilla ', 'Union ', 'Average Land Level ', \n",
    "       'Unnamed: 8', 'Longitude ', 'Latitude '], axis=1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Bridges</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load bridges\n",
    "bridges = pd.read_excel('../data/raw/BMMS_overview.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_road_segment_of_bridge(bridge):\n",
    "    \"\"\"Find road segment of bridge\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bridge : pandas.Series\n",
    "        Series containing bridge data\n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "        link_no of road segment\n",
    "    \"\"\"\n",
    "    \n",
    "    bridge_chainage = bridge['km']\n",
    "    bridge_road_name = bridge['road']\n",
    "    mask = (bridge_chainage >= road_segments['Chainage_Start'])&(bridge_chainage <= road_segments['Chainage_End']) & (road_segments['road_name'] == bridge_road_name)\n",
    "    matching_rows = road_segments[mask]\n",
    "    \n",
    "    if not matching_rows.empty:        \n",
    "            return matching_rows.iloc[0]['link_no']\n",
    "        \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bridges['link_no'] = bridges.apply(find_road_segment_of_bridge, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20415 entries, 0 to 20414\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   road              20415 non-null  object \n",
      " 1   km                20415 non-null  float64\n",
      " 2   type              20415 non-null  object \n",
      " 3   LRPName           20415 non-null  object \n",
      " 4   name              20100 non-null  object \n",
      " 5   length            20406 non-null  float64\n",
      " 6   condition         20415 non-null  object \n",
      " 7   structureNr       20415 non-null  int64  \n",
      " 8   roadName          20415 non-null  object \n",
      " 9   chainage          20415 non-null  float64\n",
      " 10  width             17408 non-null  float64\n",
      " 11  constructionYear  17407 non-null  float64\n",
      " 12  spans             17408 non-null  float64\n",
      " 13  zone              20415 non-null  object \n",
      " 14  circle            20415 non-null  object \n",
      " 15  division          20415 non-null  object \n",
      " 16  sub-division      20415 non-null  object \n",
      " 17  lat               20415 non-null  float64\n",
      " 18  lon               20415 non-null  float64\n",
      " 19  EstimatedLoc      20415 non-null  object \n",
      " 20  link_no           3833 non-null   object \n",
      "dtypes: float64(8), int64(1), object(12)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "bridges.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter bridges with link_no hence, on relevant N roads\n",
    "n_road_bridges = bridges[(bridges['link_no'].notna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_road_bridges_with_traffic = n_road_bridges.merge(road_segments_water,'left','link_no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "n_road_bridges_with_traffic.to_csv('../data/processed/bridges_with_traffic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_bridge_types_in_segment(segment,condition):\n",
    "    \"\"\"Count number of bridges in segment with given condition\n",
    "    Parameters\n",
    "    ----------\n",
    "    segment : pandas.Series\n",
    "        Series containing segment data\n",
    "        condition : string\n",
    "            condition of bridge\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        number of bridges in segment with given condition\n",
    "    \"\"\"\n",
    "    bridges = n_road_bridges_with_traffic[(n_road_bridges_with_traffic['link_no'] == segment['link_no']) & (n_road_bridges_with_traffic['condition'] == condition)]\n",
    "    return len(bridges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_segments_water['count_a_bridges'] = road_segments_water.apply(lambda segment: count_bridge_types_in_segment(segment,'A'),axis=1)\n",
    "road_segments_water['count_b_bridges'] = road_segments_water.apply(lambda segment: count_bridge_types_in_segment(segment,'B'),axis=1)\n",
    "road_segments_water['count_c_bridges'] = road_segments_water.apply(lambda segment: count_bridge_types_in_segment(segment,'C'),axis=1)\n",
    "road_segments_water['count_d_bridges'] = road_segments_water.apply(lambda segment: count_bridge_types_in_segment(segment,'D'),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_sensitivity(segment):\n",
    "    \"\"\" calculate sensitivity of segment\n",
    "    Parameters\n",
    "    ----------\n",
    "    segment : pandas.Series\n",
    "        Series containing segment data\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        sensitivity of segment\n",
    "      \"\"\"\n",
    "    \n",
    "    return (0.19*segment['count_a_bridges']+0.44*segment['count_b_bridges']+0.71*segment['count_c_bridges']+segment['count_d_bridges'])/(segment['Chainage_End']-segment['Chainage_Start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_adaptive_capacity(segment):\n",
    "    \"\"\"\n",
    "    calculate adaptive capacity of segment\n",
    "    Parameters\n",
    "    ----------\n",
    "    segment : pandas.Series\n",
    "        Series containing segment data\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        adaptive capacity of segment\n",
    "    \"\"\"\n",
    "    \n",
    "    bridges = n_road_bridges_with_traffic[(n_road_bridges_with_traffic['link_no'] == segment['link_no']) ]\n",
    "    if not bridges.empty:\n",
    "        #sum bridge length divided by longest bridge of network to the power of 1.2\n",
    "        return sum((bridges['length']/1786)**1.2)/(segment['Chainage_End']-segment['Chainage_Start'])\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate sensitivity and adaptive capacity\n",
    "road_segments_water['sensitivity'] = road_segments_water.apply(segment_sensitivity,axis=1)\n",
    "road_segments_water['adaptive_capacity'] = road_segments_water.apply(segment_adaptive_capacity,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize sensitivity, excess water level, and adaptive capacity\n",
    "road_segments_water['sensitivity_normalized'] = (road_segments_water['sensitivity']-road_segments_water['sensitivity'].min())/(road_segments_water['sensitivity'].max()-road_segments_water['sensitivity'].min())\n",
    "road_segments_water['adaptive_capacity_normalized'] = (road_segments_water['adaptive_capacity']-road_segments_water['adaptive_capacity'].min())/(road_segments_water['adaptive_capacity'].max()-road_segments_water['adaptive_capacity'].min())\n",
    "road_segments_water['exposure_normalized'] = (road_segments_water['Excess Water Level']-road_segments_water['Excess Water Level'].min())/(road_segments_water['Excess Water Level'].max()-road_segments_water['Excess Water Level'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate vulnerability index\n",
    "road_segments_water['vulnerability_index'] = 1/3*road_segments_water['sensitivity_normalized']+1/3*road_segments_water['adaptive_capacity_normalized']+1/3*road_segments_water['exposure_normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out segments with no vulnerability index\n",
    "road_segments_water = road_segments_water[road_segments_water['vulnerability_index'].notna()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Criticality</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tonnage_dict = {'Motor Cycle': 0.04,\n",
    " 'Auto Rickshaw': 0.17,\n",
    " 'Car': 0.19,\n",
    " 'Micro Bus': 0.53,\n",
    " 'Medium Bus': 2.85,\n",
    " 'Large Bus': 4.9,\n",
    " 'Small Truck': 5.0,\n",
    " 'Medium Truck': 14.0,\n",
    " 'Heavy Truck': 28.0,\n",
    " 'Utility': 0.31,\n",
    " 'Cycle Rickshaw': 0.1,\n",
    " 'Bi-Cycle': 0.02,\n",
    " 'Cart': 0.15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate total tonnage for each row based on count and values\n",
    "def calculate_tonnage(row):\n",
    "    \"\"\"calculate total tonnage for each row based on count and values\n",
    "    Parameters\n",
    "    ----------\n",
    "    segment : pandas.Series\n",
    "        Series containing segment data\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "       total tonnage for each row based on count and values of tonnage_dict\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    total_tonnage = 0\n",
    "    for mode, count in row.items():\n",
    "         if mode in tonnage_dict:\n",
    "            total_tonnage += count * tonnage_dict[mode]\n",
    "    return total_tonnage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_segments_water_tonnage = road_segments_water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate total tonnage for each row based on count and values of tonnage_dict\n",
    "road_segments_water_tonnage['total_tonnage'] = road_segments_water.apply(calculate_tonnage, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average tonnage per road name and save it in a new column 'avg_tonnage'\n",
    "road_segments_water_tonnage['avg_tonnage_road'] = road_segments_water_tonnage.groupby('road_name')['total_tonnage'].transform('mean')\n",
    "#weighted total tonnage\n",
    "road_segments_water_tonnage['weighted_total_tonnage'] = road_segments_water_tonnage['total_tonnage']/road_segments_water_tonnage['avg_tonnage_road']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize weighted total tonnage in criticality\n",
    "min_value = road_segments_water_tonnage['weighted_total_tonnage'].min()\n",
    "max_value = road_segments_water_tonnage['weighted_total_tonnage'].max()\n",
    "road_segments_water_tonnage['criticality_index'] = (road_segments_water_tonnage['weighted_total_tonnage'] - min_value) / (max_value - min_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importance index as average of vulnerability and criticality\n",
    "road_segments_water_tonnage['importance_index'] = (road_segments_water_tonnage['vulnerability_index']+road_segments_water_tonnage['criticality_index'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data to processed folder as csv\n",
    "road_segments_water_tonnage.to_csv('../data/processed/processed_road_segments.csv',index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc0f8570546888f8ce8bc6d01ace0f5b2aad11864bda16830580d8b55f2ba491"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
